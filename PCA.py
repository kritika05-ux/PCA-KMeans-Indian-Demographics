# -*- coding: utf-8 -*-
"""PCA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11uZVGNEXom43vKMQ7VmjjPxr5Bn8nFYE
"""

# Importing necessary libraries
import pandas as pd
import numpy as np

import os
if os.path.exists('census2011.csv'):
    df = pd.read_csv('census2011.csv')
else:
    # Download from URL or GitHub
    pass

df = pd.read_csv('/content/census2011.csv')

# Display the first five rows to understand the structure
print("First few rows of the dataset:")
print(df.head())

df.shape

df.isnull().sum()

#Import libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Drop irrelevant columns
# 'Ranking' is not useful for analysis
df.drop(columns=['Ranking'], inplace=True)


# Clean 'Population' column
# Remove commas and convert to numeric
df['Population'] = df['Population'].str.replace(",", "")
df['Population'] = pd.to_numeric(df['Population'], errors='coerce')

# -----------------------------------------
# Clean 'Growth' column
# Remove '%' and convert to float
df['Growth'] = df['Growth'].str.replace("%", "")
df['Growth'] = pd.to_numeric(df['Growth'], errors='coerce')


# Check for NaNs and handle them
# There should be none, but we fill with column mean just in case
df[['Population', 'Growth']] = df[['Population', 'Growth']].fillna(df[['Population', 'Growth']].mean())

# Final list of features to use in PCA
features = ['Population', 'Growth', 'Sex-Ratio', 'Literacy']
df_features = df[features]  # for PCA

# Save labels for interpretation and plots
df_labels = df[['District', 'State']]

# Final check
print("Shape of numeric feature matrix:", df_features.shape)
print("Any NaNs left?", df_features.isnull().values.any())
print("First few rows:\n", df_features.head())

df.head()

"""## **Task 1: Principal Component Analysis (PCA)**"""

import numpy as np
import matplotlib.pyplot as plt


# To standardize the features manually

X = df_features.copy()

# Z-score normalization: (x - mean) / std
X_standardized = (X - X.mean()) / X.std()


#To compute covariance matrix

X_centered = X_standardized  # already centered by z-scoring
cov_matrix = np.cov(X_centered.T)
cov_matrix = (cov_matrix + cov_matrix.T) / 2  # ensures symmetry


# For Eigenvalues and eigenvectors

eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

# Sort in descending order
sorted_indices = np.argsort(eigenvalues)[::-1]
eigenvalues_sorted = eigenvalues[sorted_indices]
eigenvectors_sorted = eigenvectors[:, sorted_indices]


#  Explained variance and thresholds

explained_variance_ratio = eigenvalues_sorted / np.sum(eigenvalues_sorted)
cumulative_variance = np.cumsum(explained_variance_ratio)

# Thresholds
thresholds = [0.80, 0.90, 0.95]
components_needed = {}
for t in thresholds:
    n = np.argmax(cumulative_variance >= t) + 1
    components_needed[f'{int(t*100)}%'] = n

# Print result
print(" Number of principal components needed to retain:")
for k, v in components_needed.items():
    print(f" - {k} variance: {v} components")


#  Plot cumulative variance

plt.figure(figsize=(8, 6))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')
plt.axhline(0.80, color='green', linestyle='--', label='80% Variance')
plt.axhline(0.90, color='orange', linestyle='--', label='90% Variance')
plt.axhline(0.95, color='red', linestyle='--', label='95% Variance')
plt.title('Cumulative Explained Variance (Standardized Features)')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# Project data onto first 2 principal components
X_pca_2d = X_standardized.values @ eigenvectors_sorted[:, :2]
print(f"Shape of projected data: {X_pca_2d.shape}")

plt.figure(figsize=(8, 5))
plt.bar(range(1, len(eigenvalues_sorted)+1), eigenvalues_sorted)
plt.xlabel('Principal Component')
plt.ylabel('Eigenvalue')
plt.title('Scree Plot')
plt.show()

def biplot(X_pca, loadings, feature_names):
    plt.figure(figsize=(8, 6))
    plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)

    # Plot loading vectors
    for i, feature in enumerate(feature_names):
        plt.arrow(0, 0, loadings[i, 0]*3, loadings[i, 1]*3,
                  color='r', alpha=0.5, head_width=0.1)
        plt.text(loadings[i, 0]*3.2, loadings[i, 1]*3.2,
                 feature, color='r', fontsize=12)

    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.title('PCA Biplot')
    plt.grid(True)
    plt.show()

# Call it after PCA
biplot(X_pca_2d, eigenvectors_sorted[:, :2], features)

"""## **Task 2: K-Means Clustering**"""

import numpy as np

def euclidean_distance(a, b):
    """
    Compute Euclidean distance between two arrays a and b.
    a: shape (n_features,)
    b: shape (m, n_features) or (n_features,)
    Returns distance(s).
    """
    return np.sqrt(np.sum((a - b) ** 2, axis=-1))

def manhattan_distance(a, b):
    """
    Compute Manhattan (L1) distance between two arrays a and b.
    """
    return np.sum(np.abs(a - b), axis=-1)

def initialize_centroids(X, K, random_state=None):
    """
    Randomly initialize centroids by selecting K distinct samples from X.
    """
    if random_state:
        np.random.seed(random_state)
    indices = np.random.choice(X.shape[0], size=K, replace=False)
    return X[indices]

def assign_clusters(X, centroids, distance_metric):
    """
    Assign each data point in X to the closest centroid based on the distance metric.
    Returns an array of cluster indices for each sample.
    """
    distances = np.zeros((X.shape[0], centroids.shape[0]))
    for i, centroid in enumerate(centroids):
        distances[:, i] = distance_metric(X, centroid)
    return np.argmin(distances, axis=1)

def update_centroids(X, labels, K):
    """
    Update centroids by computing the mean of all points assigned to each cluster.
    """
    new_centroids = np.zeros((K, X.shape[1]))
    for k in range(K):
        cluster_points = X[labels == k]
        if len(cluster_points) > 0:
            new_centroids[k] = cluster_points.mean(axis=0)
        else:
            # If no points assigned to cluster, reinitialize centroid randomly
            new_centroids[k] = X[np.random.choice(X.shape[0])]
    return new_centroids

def kmeans(X, K, distance_metric, max_iters=300, tol=1e-4, random_state=None):
    """
    K-Means clustering algorithm.
    Returns:
        labels: cluster assignment for each sample
        centroids: final centroids
        iterations: number of iterations run until convergence
    """
    centroids = initialize_centroids(X, K, random_state)
    for iteration in range(1, max_iters + 1):
        labels = assign_clusters(X, centroids, distance_metric)
        new_centroids = update_centroids(X, labels, K)

        # Check convergence by centroid movement (tolerance)
        centroid_shifts = np.linalg.norm(new_centroids - centroids, axis=1)
        if np.all(centroid_shifts <= tol):
            break
        centroids = new_centroids

    return labels, centroids, iteration

# Run K-Means for combinations of K and distance metrics


# Use standardized features X_standardized from PCA task
X = X_standardized.values  # convert DataFrame to numpy array

Ks = [5, 7, 9]
distance_metrics = {
    'Euclidean': euclidean_distance,
    'Manhattan': manhattan_distance
}

results = []

for metric_name, metric_func in distance_metrics.items():
    for K in Ks:
        labels, centroids, iterations = kmeans(X, K, metric_func, random_state=42)
        results.append({
            'K': K,
            'Distance Metric': metric_name,
            'Iterations to Converge': iterations
        })

# Print the results neatly
print("K-Means Clustering Results (Iterations to Convergence):")
for res in results:
    print(f"K={res['K']}, Distance Metric={res['Distance Metric']} -> Iterations: {res['Iterations to Converge']}")

"""## **Task 3: Visualization and Interpretation**"""

# Utility: Plot cluster assignments in PCA space

def plot_clusters(X_2d, labels, title):
    plt.figure(figsize=(8, 6))
    scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels, cmap='tab10', s=50, alpha=0.8)
    plt.xlabel("Principal Component 1")
    plt.ylabel("Principal Component 2")
    plt.title(title)
    plt.grid(True)
    plt.colorbar(scatter, label='Cluster ID')
    plt.tight_layout()
    plt.show()


# Utility: Compute mean values of original features per cluster

def describe_clusters(original_df, labels, K):
    df = original_df.copy()
    df['Cluster'] = labels
    summary = df.groupby('Cluster').mean().round(2)
    return summary

# Run clustering and describe for each combination

Ks = [5, 7, 9]
metrics = {
    "Euclidean": euclidean_distance,
    "Manhattan": manhattan_distance
}

# PCA-projected data (from previous PCA step)
X_2d = X_pca_2d
X_original = df_features.copy()

for metric_name, metric_func in metrics.items():
    for K in Ks:
        labels, _, _ = kmeans(X, K, metric_func, random_state=42)

        # Plot clusters in PCA space
        plot_title = f"K={K} Clusters ({metric_name}) - PCA Projection"
        plot_clusters(X_2d, labels, plot_title)

        # Describe cluster profiles
        summary = describe_clusters(X_original, labels, K)
        print(f"\nðŸ“Š Cluster Summary: K={K}, Distance={metric_name}")
        print(summary)

"""## **Concise Description of Each Segmentâ€™s Defining Characteristics**

**Interpretation: K=5 Clusters (Euclidean)**

**Cluster , 	 Population ,	 Growth,	  Sex-Ratio,	   Literacy**



**0	   ,         4.6M	 ,     19.23	 ,  939   	,      72.8%**

Interpretation-
Large-population districts with moderate growth and literacy. Likely urban or peri-urban regions with male-skewed demographics.

**1	   ,     1.4M	  ,   22.19%	 ,   955	  ,     62.1%**

Interpretation-
Rural districts with high growth but low literacy, possibly underdeveloped or backward regions.

**2     ,  	1.3M	 ,   16.98%	  ,  885	   ,    74.4%**

Interpretation-
Moderate-size districts with low sex ratio and moderate literacy. Suggests patriarchal bias with improving education.

**3	     ,   290K	 ,   88.03%	 ,   908	   ,    76.7%	**

Interpretation-
Explosively growing small districts â€” possible new administrative zones or urban expansion fringes.

**4	    ,   1.3M	 ,     11.82%	 ,   996	   ,    81.6%**

Interpretation-
Well-developed, stable districts with high literacy and balanced gender ratio. Possibly southern or progressive states.

**Interpretation: K=7 Clusters (Euclidean)**

**Cluster  ,	Population ,	Growth ,	Sex-Ratio  , 	Literacy**

**0	 , 6.7M	  , 21.52%  , 	920  , 	81.4%**

Interpretation-
Highly populated and educated districts, likely major metros or capitals. Balanced in growth and gender.

**1	  , 1.3M  ,	8.87%  ,	1029  ,	87.4%**
interpretation-
High sex-ratio and literacy with low growth â€” likely female-friendly, developed regions (e.g., Kerala, NE states).

**2	,  3.75M  ,	18.91%	, 945	 , 69.0%**

Interpretation-
Large-population districts with average development, possibly industrial belts.

**3	, 290K ,	88.03%	 , 908 , 	76.7%**

Interpretation- Same as K=5: Small districts with very high growth, suggesting administrative expansion areas.

**4 ,	1.35M ,	16.50% ,	882	 , 76.1%**

Interpretation-
Low sex-ratio districts, possibly northern belt, with improving literacy.

**5 ,	1.29M ,	15.62% ,	971 ,	73.4%**

Interpretation-Balanced gender ratio and literacy. Mid-tier rural or semi-urban regions.

**6 ,	1.25M ,	24.34% ,	943 ,	58.9%**

Interpretation- High-growth, low-literacy districts, likely in lagging states like Bihar or UP.

**Interpretation: K=9, Euclidean**

**Cluster ,	Interpretation -**

**0	-** Very large districts with high growth and high literacy, suggesting rapidly expanding urban areas.

**1-**Small-medium districts with very high sex ratio and literacy but low growth â€” typical of female-favorable, stable regions (e.g., Kerala, Northeast).

**2-** High-growth, lower-literacy urban-industrial zones, possibly tier-2 cities.

**3-**	Very small districts with extreme growth â€” potentially new or split districts. Very dynamic.

**4-**	Low sex ratio and average literacy, possible patriarchal and moderately developed areas.

**5-**	High population with good sex ratio and average literacy â€” possible large, balanced districts.

**6-**	Typical mid-tier districts with above-average sex ratio, modest growth, and moderate literacy.

**7-**	High growth, very low literacy â€” indicative of backward rural regions (e.g., Bihar, MP).

**8-**	Small, fast-growing, low-literacy and extremely low sex-ratio regions â€” likely neglected or remote zones.

**Interpretation: K=5, Manhattan**

**Cluster	,Interpretation**

**0-**	Large, moderately developed districts with balanced growth and literacy. Possibly older metros.

**1-**	Smaller regions with high sex ratio, but low literacy â€” possibly rural but demographically healthier.

**2-**	Low sex ratio, average literacy â€” possibly northern patriarchal regions.

**3-**	Small districts with very high growth and balanced literacy â€” similar to newly established zones.

**4-**	Educated, balanced districts â€” high literacy and healthy gender ratio, low growth. Likely well-governed areas

**Interpretation: K=7, Manhattan**

**Cluster	, Interpretation**

**0-**	High population, educated metros with moderate gender balance. Possibly state capitals.

**1-**	Small-medium regions with very high literacy and sex ratio â€” best performing areas.

**2	-**High population, moderate development, literacy slightly lagging â€” industrial zones or dense districts.

**3-**	Small districts with very high growth, typical of administrative expansion areas.

**4-**	Educated and moderately developed regions with low sex ratios.

**5-**	Average population and development; moderate in every aspect â€” typical semi-urban belt.

**6-**	High-growth, low-literacy districts â€” probably in BIMARU (Bihar, MP, etc.) states.

**Interpretation: K=9, Manhattan**

**Cluster ,	Interpretation**

**0-**	High population, healthy gender ratio, moderately literate â€” large but developing regions.

**1-**	Very small regions with extremely high literacy and sex ratio â€” top performers (e.g., Kerala, Mizoram).

**2-**	Large, fast-growing, lower-literacy â€” urban belts with educational needs.

**3-**	Extremely small, explosively growing regions â€” likely administrative splits or special zones.

**4-**	Very low sex ratio, average literacy â€” male-dominant rural districts.

**5-**	Medium population and moderate development â€” balanced rural/urban fringe districts.

**6-**	Low population, high literacy â€” quiet but developed areas.

**7-**	High-growth, low-literacy districts â€” underdeveloped zones with social challenges.

**8-**	Small, fast-growing, low sex ratio regions â€” neglected districts possibly needing intervention.

# **THANK YOU**
"""